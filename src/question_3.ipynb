{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL59neM0-pJg"
   },
   "source": [
    "Q3. Generate a set X1 that consists of N1 = 50 5-dimensional data vectors that stem from two equiprobable classes, ω1 and ω2. The classes are modelled by Gaussian distributions with means m1 = [0,0,0,0,0]T  and m2 = [1,1,1,1,1]T  and respective covariance matrices \n",
    "\n",
    ">$S1 = \\begin{bmatrix}\n",
    "0.8 &  0.2 & 0.1 & 0.05 & 0.01\\\\\n",
    "0.2 &  0.7 & 0.1 & 0.03 & 0.02 \\\\\n",
    "0.1 & 0.1 & 0.8 & 0.02 & 0.01 \\\\\n",
    "0.05 & 0.03 & 0.02 & 0.9 & 0.01 \\\\\n",
    "0.01 & 0.02 & 0.01 & 0.01 & 0.8 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    ">$S2 = \\begin{bmatrix}\n",
    "0.9 &  0.1 & 0.05 & 0.02 & 0.01\\\\\n",
    "0.1 &  0.8 & 0.1 & 0.02 & 0.02 \\\\\n",
    "0.05 & 0.1 & 0.7 & 0.02 & 0.01 \\\\\n",
    "0.02 & 0.02 & 0.02 & 0.6 & 0.02 \\\\\n",
    "0.01 & 0.02 & 0.01 & 0.02 & 0.7 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "In a similar manner, generate a data set X2 consisting of N2 = 10,000 data points. X1 is \n",
    "used for training; X2, for testing. In the spirit of the naive Bayes classifier, we assume that for each class the features of the feature vectors are statistically independent and that each follows a 1-dimensional Gaussian distribution. For each of the five dimensions and for each of the two classes, the mean values are m1j, m2j, j = 1, 2, . . . \n",
    ",5 and the variances are σ21j, σ22j, j = 1, 2, . . .,5. \n",
    "Classify the points of the test set X2 using the naive Bayes classifier, where for a given x, p(x|ωi ) is estimated as\n",
    "\n",
    ">$\n",
    "p(x \\mid \\omega_i)=\\Pi_{j=1}^5 \\frac{1}{\\sqrt{2\\pi\\sigma^2_{ij}}}\\,e^{ -\\frac{(x(j)-m_{ij})^2}{2\\sigma^2_{ij}} }, i=1,2\n",
    "$\n",
    "\n",
    "where x( j) is the jth component of x. Compute the error probability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1732,
     "status": "ok",
     "timestamp": 1609689234786,
     "user": {
      "displayName": "Ajay Biswas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1iLIxFozPtsIbKxgUe8FI8f0sNDeaxtFpfHY-=s64",
      "userId": "11556512077135312826"
     },
     "user_tz": -330
    },
    "id": "6GBOUi4jKz3i",
    "outputId": "cd577528-6939-4c01-aec6-66196e7333e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated means of the two classes:\n",
      " [[ 0.01928201  0.08783246 -0.07484211 -0.14611728  0.17308157]\n",
      " [ 1.01484459  0.84843094  0.75732314  0.96103605  0.95368039]]\n",
      "\n",
      "Estimated variances of the two classes:\n",
      " [[0.665506   0.55596771 0.80906563 0.73307149 0.82873824]\n",
      " [0.67931774 0.99258799 0.66529146 0.56037065 0.59827908]]\n",
      "\n",
      "No. of Misclassifications:  1423\n",
      "\n",
      "Error Probability:  0.14229999999999998\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gaussian naive bayes classifier\n",
    "\n",
    "@Author: Ajay Biswas\n",
    "220CS2184\n",
    "National Institute of Technology, Rourkela\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import Counter  \n",
    "import math\n",
    "\n",
    "def misclassifications(X,Y):\n",
    "  correct_count = 0\n",
    "  for i in range(len(X)):\n",
    "    if(X[i] == Y[i]):\n",
    "      correct_count = correct_count + 1\n",
    "  return len(X) - correct_count\n",
    "\n",
    "def MER_Error(X,Y):\n",
    "  correct_count = 0\n",
    "  for i in range(len(X)):\n",
    "    if(X[i] == Y[i]):\n",
    "      correct_count = correct_count + 1\n",
    "  MER_val = 1 - (correct_count/len(X))\n",
    "  return MER_val\n",
    "\n",
    "def estimator(X,V,Y):  \n",
    "   means = X\n",
    "   variances = V\n",
    "   no_features = len(means)\n",
    "   p = 1\n",
    "   for i in range(no_features):\n",
    "       exponent =  math.exp(-((Y[i] - means[i]) ** 2 / (2 * variances[i])))\n",
    "       fraction = (1 / (math.sqrt(2 * math.pi * variances[i])))\n",
    "       p = p * exponent*fraction     \n",
    "   return p\n",
    "       \n",
    "\n",
    "def naive_bayes_train(X,y):\n",
    "    # X contains n dimensional features\n",
    "    # y contains true label\n",
    "    # find mean of X\n",
    "    \n",
    "    no_of_samples,no_of_features = X.shape\n",
    "        \n",
    "    unique_classes = set(y)\n",
    "    sc = dict(Counter(y))\n",
    "    frequency_per_class = [sc[i] for i in unique_classes]\n",
    "    \n",
    "    # group samples classwise and find their centroid\n",
    "    start = 0\n",
    "    means = []\n",
    "    variances = []\n",
    "    for value in frequency_per_class:\n",
    "        tempList = X[start:start+value,:]\n",
    "        each_mean = tempList.mean(0)\n",
    "        each_variance = tempList.var(0)\n",
    "        means.append(each_mean)\n",
    "        variances.append(each_variance)\n",
    "        start = value\n",
    "        \n",
    "    return np.array(means),np.array(variances)\n",
    "\n",
    "def naive_bayes_test(X,means,variances,y):\n",
    "    # X contains n dimensional features\n",
    "    # y contains true class labels\n",
    "    # model is array of centroids for each class present in y respectively\n",
    "    \n",
    "    num_rows, num_cols = means.shape\n",
    "    X_rows, X_cols = X.shape\n",
    "    unique_classes = set(y)\n",
    "    y_len = len(y)\n",
    "    \n",
    "    # no. of labels must match no. of rows in the model (array of centroids)\n",
    "    if(len(unique_classes)!= num_rows):\n",
    "        return None\n",
    "    \n",
    "    # no. of features must match with the no. of dimensions of the model\n",
    "    if(X_cols != num_cols):\n",
    "        return None\n",
    "    \n",
    "    # predicted labels\n",
    "    predicted = [0]*y_len\n",
    "   \n",
    "    # test each point against each class and assign label based on max probability\n",
    "    prob = 0\n",
    "    max_prob = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for test_point in X:\n",
    "        j = 0\n",
    "        for each_mean,each_variance in means,variances:\n",
    "            prob = estimator(each_mean,each_variance,test_point)\n",
    "            \n",
    "            if(max_prob < prob):\n",
    "                max_prob = prob\n",
    "                label = j\n",
    "            j+=1\n",
    "          \n",
    "        predicted[i] = label    \n",
    "        max_prob = 0\n",
    "        i+=1\n",
    "        \n",
    "    return np.array(predicted) \n",
    "\n",
    "\n",
    "def main():\n",
    "    S1 =np.array([[0.8, 0.2, 0.1, 0.05, 0.01],\n",
    "        [0.2, 0.7, 0.1, 0.03, 0.02],\n",
    "        [0.1, 0.1, 0.8, 0.02, 0.01],\n",
    "        [0.05, 0.03, 0.02, 0.9, 0.01],\n",
    "        [0.01, 0.02, 0.01, 0.01, 0.8]])\n",
    "    \n",
    "    S2 =np.array([[0.9, 0.1, 0.05, 0.02, 0.01],\n",
    "        [0.1, 0.8, 0.1, 0.02, 0.02],\n",
    "        [0.05, 0.1, 0.7, 0.02, 0.01],\n",
    "        [0.02, 0.02, 0.02, 0.6, 0.02],\n",
    "        [0.01, 0.02, 0.01, 0.02, 0.7]])\n",
    "    mean = [0,0,0,0,0]\n",
    "    mean2 = [1,1,1,1,1]\n",
    "    \n",
    "    # training\n",
    "    X_h1 = np.random.multivariate_normal(mean, S1, 25)\n",
    "    X_h2 = np.random.multivariate_normal(mean2, S2, 25)\n",
    "    X = np.concatenate((X_h1,X_h2))\n",
    "    y = np.concatenate(([0]*25,[1]*25))    \n",
    "    means,variances = naive_bayes_train(X,y)\n",
    "\n",
    "    print('\\nEstimated means of the two classes:\\n',means)\n",
    "    print('\\nEstimated variances of the two classes:\\n',variances)\n",
    "    \n",
    "    # testing\n",
    "    X2_h1 = np.random.multivariate_normal(mean, S1, 5000)\n",
    "    X2_h2 = np.random.multivariate_normal(mean2, S2, 5000)\n",
    "    X2 = np.concatenate((X2_h1,X2_h2))  \n",
    "    y2 = np.concatenate(([0]*5000,[1]*5000))    \n",
    "    L = naive_bayes_test(X2,means,variances,y2)  \n",
    "    \n",
    "    error_rate = MER_Error(y2, L)\n",
    "    misclassification =  misclassifications(y2, L)\n",
    "\n",
    "    print('\\nNo. of Misclassifications: ',misclassification)\n",
    "    print('\\nError Probability: ',error_rate)\n",
    "\n",
    "if __name__==\"__main__\": \n",
    "    main() \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ques3_naive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
