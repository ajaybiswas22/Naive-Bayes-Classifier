{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ques3_naive.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DL59neM0-pJg"},"source":["Q3. Generate a set X1 that consists of N1 = 50 5-dimensional data vectors that stem from two equiprobable classes, ω1 and ω2. The classes are modelled by Gaussian distributions with means m1 = [0,0,0,0,0]T  and m2 = [1,1,1,1,1]T  and respective covariance matrices \r\n","\r\n",">$S1 = \\begin{bmatrix}\r\n","0.8 &  0.2 & 0.1 & 0.05 & 0.01\\\\\r\n","0.2 &  0.7 & 0.1 & 0.03 & 0.02 \\\\\r\n","0.1 & 0.1 & 0.8 & 0.02 & 0.01 \\\\\r\n","0.05 & 0.03 & 0.02 & 0.9 & 0.01 \\\\\r\n","0.01 & 0.02 & 0.01 & 0.01 & 0.8 \\\\\r\n","\\end{bmatrix}$\r\n","\r\n",">$S2 = \\begin{bmatrix}\r\n","0.9 &  0.1 & 0.05 & 0.02 & 0.01\\\\\r\n","0.1 &  0.8 & 0.1 & 0.02 & 0.02 \\\\\r\n","0.05 & 0.1 & 0.7 & 0.02 & 0.01 \\\\\r\n","0.02 & 0.02 & 0.02 & 0.6 & 0.02 \\\\\r\n","0.01 & 0.02 & 0.01 & 0.02 & 0.7 \\\\\r\n","\\end{bmatrix}$\r\n","\r\n","In a similar manner, generate a data set X2 consisting of N2 = 10,000 data points. X1 is \r\n","used for training; X2, for testing. In the spirit of the naive Bayes classifier, we assume that for each class the features of the feature vectors are statistically independent and that each follows a 1-dimensional Gaussian distribution. For each of the five dimensions and for each of the two classes, the mean values are m1j, m2j, j = 1, 2, . . . \r\n",",5 and the variances are σ21j, σ22j, j = 1, 2, . . .,5. \r\n","Classify the points of the test set X2 using the naive Bayes classifier, where for a given x, p(x|ωi ) is estimated as\r\n","\r\n",">$\r\n","p(x \\mid \\omega_i)=\\Pi_{j=1}^5 \\frac{1}{\\sqrt{2\\pi\\sigma^2_{ij}}}\\,e^{ -\\frac{(x(j)-m_{ij})^2}{2\\sigma^2_{ij}} }, i=1,2\r\n","$\r\n","\r\n","where x( j) is the jth component of x. Compute the error probability. \r\n"]},{"cell_type":"code","metadata":{"id":"6GBOUi4jKz3i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609689234786,"user_tz":-330,"elapsed":1732,"user":{"displayName":"Ajay Biswas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1iLIxFozPtsIbKxgUe8FI8f0sNDeaxtFpfHY-=s64","userId":"11556512077135312826"}},"outputId":"cd577528-6939-4c01-aec6-66196e7333e4"},"source":["\"\"\"\r\n","Gaussian naive bayes classifier\r\n","\r\n","@Author: Ajay Biswas\r\n","220CS2184\r\n","National Institute of Technology, Rourkela\r\n","\"\"\"\r\n","\r\n","import numpy as np\r\n","from sklearn.naive_bayes import GaussianNB\r\n","from collections import Counter  \r\n","import math\r\n","\r\n","def misclassifications(X,Y):\r\n","  correct_count = 0\r\n","  for i in range(len(X)):\r\n","    if(X[i] == Y[i]):\r\n","      correct_count = correct_count + 1\r\n","  return len(X) - correct_count\r\n","\r\n","def MER_Error(X,Y):\r\n","  correct_count = 0\r\n","  for i in range(len(X)):\r\n","    if(X[i] == Y[i]):\r\n","      correct_count = correct_count + 1\r\n","  MER_val = 1 - (correct_count/len(X))\r\n","  return MER_val\r\n","\r\n","def estimator(X,V,Y):  \r\n","   means = X\r\n","   variances = V\r\n","   no_features = len(means)\r\n","   p = 1\r\n","   for i in range(no_features):\r\n","       exponent =  math.exp(-((Y[i] - means[i]) ** 2 / (2 * variances[i])))\r\n","       fraction = (1 / (math.sqrt(2 * math.pi * variances[i])))\r\n","       p = p * exponent*fraction     \r\n","   return p\r\n","       \r\n","\r\n","def naive_bayes_train(X,y):\r\n","    # X contains n dimensional features\r\n","    # y contains true label\r\n","    # find mean of X\r\n","    \r\n","    no_of_samples,no_of_features = X.shape\r\n","        \r\n","    unique_classes = set(y)\r\n","    sc = dict(Counter(y))\r\n","    frequency_per_class = [sc[i] for i in unique_classes]\r\n","    \r\n","    # group samples classwise and find their centroid\r\n","    start = 0\r\n","    means = []\r\n","    variances = []\r\n","    for value in frequency_per_class:\r\n","        tempList = X[start:start+value,:]\r\n","        each_mean = tempList.mean(0)\r\n","        each_variance = tempList.var(0)\r\n","        means.append(each_mean)\r\n","        variances.append(each_variance)\r\n","        start = value\r\n","        \r\n","    return np.array(means),np.array(variances)\r\n","\r\n","def naive_bayes_test(X,means,variances,y):\r\n","    # X contains n dimensional features\r\n","    # y contains true class labels\r\n","    # model is array of centroids for each class present in y respectively\r\n","    \r\n","    num_rows, num_cols = means.shape\r\n","    X_rows, X_cols = X.shape\r\n","    unique_classes = set(y)\r\n","    y_len = len(y)\r\n","    \r\n","    # no. of labels must match no. of rows in the model (array of centroids)\r\n","    if(len(unique_classes)!= num_rows):\r\n","        return None\r\n","    \r\n","    # no. of features must match with the no. of dimensions of the model\r\n","    if(X_cols != num_cols):\r\n","        return None\r\n","    \r\n","    # predicted labels\r\n","    predicted = [0]*y_len\r\n","   \r\n","    # test each point against each class and assign label based on max probability\r\n","    prob = 0\r\n","    max_prob = 0\r\n","    i = 0\r\n","    j = 0\r\n","    for test_point in X:\r\n","        j = 0\r\n","        for each_mean,each_variance in means,variances:\r\n","            prob = estimator(each_mean,each_variance,test_point)\r\n","            \r\n","            if(max_prob < prob):\r\n","                max_prob = prob\r\n","                label = j\r\n","            j+=1\r\n","          \r\n","        predicted[i] = label    \r\n","        max_prob = 0\r\n","        i+=1\r\n","        \r\n","    return np.array(predicted) \r\n","\r\n","\r\n","def main():\r\n","    S1 =np.array([[0.8, 0.2, 0.1, 0.05, 0.01],\r\n","        [0.2, 0.7, 0.1, 0.03, 0.02],\r\n","        [0.1, 0.1, 0.8, 0.02, 0.01],\r\n","        [0.05, 0.03, 0.02, 0.9, 0.01],\r\n","        [0.01, 0.02, 0.01, 0.01, 0.8]])\r\n","    \r\n","    S2 =np.array([[0.9, 0.1, 0.05, 0.02, 0.01],\r\n","        [0.1, 0.8, 0.1, 0.02, 0.02],\r\n","        [0.05, 0.1, 0.7, 0.02, 0.01],\r\n","        [0.02, 0.02, 0.02, 0.6, 0.02],\r\n","        [0.01, 0.02, 0.01, 0.02, 0.7]])\r\n","    mean = [0,0,0,0,0]\r\n","    mean2 = [1,1,1,1,1]\r\n","    \r\n","    # training\r\n","    X_h1 = np.random.multivariate_normal(mean, S1, 25)\r\n","    X_h2 = np.random.multivariate_normal(mean2, S2, 25)\r\n","    X = np.concatenate((X_h1,X_h2))\r\n","    y = np.concatenate(([0]*25,[1]*25))    \r\n","    means,variances = naive_bayes_train(X,y)\r\n","\r\n","    print('\\nEstimated means of the two classes:\\n',means)\r\n","    print('\\nEstimated variances of the two classes:\\n',variances)\r\n","    \r\n","    # testing\r\n","    X2_h1 = np.random.multivariate_normal(mean, S1, 5000)\r\n","    X2_h2 = np.random.multivariate_normal(mean2, S2, 5000)\r\n","    X2 = np.concatenate((X2_h1,X2_h2))  \r\n","    y2 = np.concatenate(([0]*5000,[1]*5000))    \r\n","    L = naive_bayes_test(X2,means,variances,y2)  \r\n","    \r\n","    error_rate = MER_Error(y2, L)\r\n","    misclassification =  misclassifications(y2, L)\r\n","\r\n","    print('\\nNo. of Misclassifications: ',misclassification)\r\n","    print('\\nError Probability: ',error_rate)\r\n","\r\n","if __name__==\"__main__\": \r\n","    main() \r\n","    \r\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\n","Estimated means of the two classes:\n"," [[ 0.01928201  0.08783246 -0.07484211 -0.14611728  0.17308157]\n"," [ 1.01484459  0.84843094  0.75732314  0.96103605  0.95368039]]\n","\n","Estimated variances of the two classes:\n"," [[0.665506   0.55596771 0.80906563 0.73307149 0.82873824]\n"," [0.67931774 0.99258799 0.66529146 0.56037065 0.59827908]]\n","\n","No. of Misclassifications:  1423\n","\n","Error Probability:  0.14229999999999998\n"],"name":"stdout"}]}]}